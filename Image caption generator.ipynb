{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image caption generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - seq2seq auto encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path('data/')\n",
    "PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_PATH = PATH/'seq2deq_model'\n",
    "seq_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_and_links_valid = pd.read_csv(\"Validation_GCC-1.1.0-Validation.tsv\", sep=\"\\t\",header=None)\n",
    "captions_and_links_train = pd.read_csv(\"Train_GCC-training.tsv\", sep=\"\\t\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15840, 3318333)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(captions_and_links_valid), len(captions_and_links_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>author : a life in photography -- in pictures</td>\n",
       "      <td>https://i.pinimg.com/736x/66/01/6c/66016c3ba27...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>an angler fishes river on a snowy day .</td>\n",
       "      <td>http://www.standard.net/image/2015/02/04/800x_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>photograph of the sign being repaired by brave...</td>\n",
       "      <td>http://indianapolis-photos.funcityfinder.com/f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the player staring intently at a computer scre...</td>\n",
       "      <td>http://www.abc.net.au/news/image/9066492-3x2-7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>globes : the green 3d person carrying in hands...</td>\n",
       "      <td>https://www.featurepics.com/StockImage/2009031...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0      author : a life in photography -- in pictures   \n",
       "1            an angler fishes river on a snowy day .   \n",
       "2  photograph of the sign being repaired by brave...   \n",
       "3  the player staring intently at a computer scre...   \n",
       "4  globes : the green 3d person carrying in hands...   \n",
       "\n",
       "                                                   1  \n",
       "0  https://i.pinimg.com/736x/66/01/6c/66016c3ba27...  \n",
       "1  http://www.standard.net/image/2015/02/04/800x_...  \n",
       "2  http://indianapolis-photos.funcityfinder.com/f...  \n",
       "3  http://www.abc.net.au/news/image/9066492-3x2-7...  \n",
       "4  https://www.featurepics.com/StockImage/2009031...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions_and_links_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_valid = captions_and_links_valid[0].values\n",
    "captions_train = captions_and_links_train[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3318333, 15840)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(captions_train), len(captions_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_captions_valid = tokenizer.process_all(captions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_captions_train = tokenizer.process_all(captions_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's make sure everything went right:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people click into their skis . people click into their skis .\n"
     ]
    }
   ],
   "source": [
    "print(captions_train[777], ' '.join(tokens_captions_train[777]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length of captions\n",
    "#### Average length of captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.415467171717172"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([len(c) for c in tokens_captions_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.322056888202601"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([len(c) for c in tokens_captions_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discard captions that are too long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_captions_valid = np.array(tokens_captions_valid)[np.array([len(c) < 30 for c in tokens_captions_valid])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max([len(c) for c in tokens_captions_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_captions_train = np.array(tokens_captions_train)[np.array([len(c) < 30 for c in tokens_captions_train])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max([len(c) for c in tokens_captions_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump((tokens_captions_valid), (seq_PATH/'tok_cap_v.pkl').open('wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump((tokens_captions_train), (seq_PATH/'tok_cap_t.pkl').open('wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_captions_valid = pickle.load((seq_PATH/'tok_cap_v.pkl').open('rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_captions_train = pickle.load((seq_PATH/'tok_cap_t.pkl').open('rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3302516 15764\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens_captions_train), len(tokens_captions_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numericalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab.create(tokens_captions_train, max_vocab=50000, min_freq=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's quickly test this:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 12575"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actor attending the world premiere of crime fiction film\n"
     ]
    }
   ],
   "source": [
    "test_caption = tokens_captions_valid[idx]\n",
    "print(' '.join(w for w in test_caption))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31, 1841, 8, 108, 43, 11, 2274, 1018, 76]\n"
     ]
    }
   ],
   "source": [
    "test_caption_num = vocab.numericalize(test_caption)\n",
    "print(test_caption_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actor attending the world premiere of crime fiction film\n"
     ]
    }
   ],
   "source": [
    "print(vocab.textify(test_caption_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['author', ':', 'a', 'life', 'in', 'photography', '--', 'in', 'pictures']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_captions_valid[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalize_tokens(tok):\n",
    "    return np.array([vocab.numericalize(q) + [1] for q in tok])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_numericalized_valid = numericalize_tokens(tokens_captions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_numericalized_train = numericalize_tokens(tokens_captions_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a vector illustration of a motorcycle contrasted against a bicycle . xxpad'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.textify(captions_numericalized_train[1256339])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install git+https://github.com/facebookresearch/fastText.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastText as ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vecs = ft.load_model(str(seq_PATH/'wiki.en.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_dict = {w : en_vecs.get_word_vector(w) for w in en_vecs.get_words()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(vec_dict, open(seq_PATH/'vec_dict.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_dict = pickle.load(open(seq_PATH/'vec_dict.pkl','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's take a look at the most frequent words from fastText:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_words = en_vecs.get_words(include_freq=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_word_dict = {k:v for k,v in zip(*ft_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_words = sorted(ft_word_dict.keys(), key=lambda x: ft_word_dict[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 most frequent words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[')', \"'\", 'and', 'in', '-', 'of', '</s>', 'the', '.', ',']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_words[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mean and stdv of the word vectors:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs = np.stack(list(vec_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0075652334, 0.29283327)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs.mean(), vecs.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def A(*a):\n",
    "    \"\"\"convert iterable object into numpy array\"\"\"\n",
    "    return np.array(a[0]) if len(a)==1 else [np.array(o) for o in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Caption2CaptionDataset(Dataset):\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "    def __getitem__(self, idx):\n",
    "        return A(self.x[idx], self.x[idx])\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_ds = Caption2CaptionDataset(captions_numericalized_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = Caption2CaptionDataset(captions_numericalized_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3302516, 15764)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trn_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a very typical bus station xxpad'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.textify(trn_ds[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_sampler = SortSampler(captions_numericalized_valid, key=lambda x:len(captions_numericalized_valid[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_sampler = SortishSampler(captions_numericalized_train, key=lambda x: len(captions_numericalized_train[x]), bs=bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to fully utilize the GPUs capabilities, we train in batches. The length of a minibatch tensor is set by the sequence length of the longest question in that batch. The other questions are padded. To save time and memory, we want to avoid very long and very short questions in one batch because that would mean lot's of padding. For the validation set we simply sort the questions. For training we use the `SortishSampler` which groups *longer* questions together and *shorter* questions together while preserving some randomness.\n",
    "\n",
    "For language models it's better to pad before the start of the sequence because we need the final hidden state to predict the next token or for classification...\n",
    "\n",
    "For sequence to sequence models it is better to pad after the end of the sequence.\n",
    "\n",
    "The samplers return an index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = next(iter(trn_sampler))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'never in my life used false eyelashes -- if i get some what brand and how in the world do i pick a kind ? my eyes are small xxpad'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.textify(captions_numericalized_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_collate_seq2seq(samples:BatchSamples, pad_idx:int=1, pad_first:bool=True, backwards:bool=False, transpose:bool=False) -> Tuple[LongTensor, LongTensor]:\n",
    "    \"Function that collect samples and adds padding. Flips token order if needed\"\n",
    "    samples = to_data(samples)\n",
    "    max_len_inp = max([len(s[0]) for s in samples])\n",
    "    max_len_out = max([len(s[1]) for s in samples])\n",
    "    \n",
    "    res_inp = torch.zeros(len(samples), max_len_inp).long() + pad_idx\n",
    "    res_out = torch.zeros(len(samples), max_len_out).long() + pad_idx\n",
    "    \n",
    "    if backwards: pad_first = not pad_first\n",
    "    for i,s in enumerate(samples):\n",
    "        if pad_first: \n",
    "            res_inp[i,-len(s[0]):] = LongTensor(s[0])\n",
    "            res_out[i,-len(s[1]):] = LongTensor(s[1])\n",
    "        else:         \n",
    "            res_inp[i,:len(s[0]):] = LongTensor(s[0])\n",
    "            res_out[i,:len(s[1]):] = LongTensor(s[1])\n",
    "    if backwards:\n",
    "        res = res.flip(1)\n",
    "    if transpose:\n",
    "        res_inp.transpose_(0,1)\n",
    "        res_out.transpose_(0,1)\n",
    "    return res_inp, res_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pad_collate_func = partial(pad_collate_seq2seq, pad_first=False, transpose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dl = DataLoader(dataset=trn_ds, batch_size=bs, sampler=trn_sampler, collate_fn=my_pad_collate_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dl = DataLoader(dataset=val_ds, batch_size=bs, sampler=val_sampler, collate_fn=my_pad_collate_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the dataloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "its = [next(it) for _ in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(torch.Size([30, 125]), torch.Size([30, 125])),\n",
       " (torch.Size([27, 125]), torch.Size([27, 125])),\n",
       " (torch.Size([24, 125]), torch.Size([24, 125]))]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(x.shape, y.shape) for x, y in its]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is an old photo of my figure i had and i slightly edited it and now he looks like he 's from a dramatic anime xxpad xxpad xxpad xxpad\n",
      "this is an old photo of my figure i had and i slightly edited it and now he looks like he 's from a dramatic anime xxpad xxpad xxpad xxpad\n",
      "\n",
      "i have no idea how that american flag got in this picture of the construction , but i love that it is there ! xxpad xxpad xxpad\n",
      "i have no idea how that american flag got in this picture of the construction , but i love that it is there ! xxpad xxpad xxpad\n",
      "\n",
      "detail shot : the hands of a woman driving a car by the countryside , seen from the front passenger 's seat xxpad xxpad\n",
      "detail shot : the hands of a woman driving a car by the countryside , seen from the front passenger 's seat xxpad xxpad\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x, y in its:\n",
    "    print(vocab.textify(x[:,100]))\n",
    "    print(vocab.textify(y[:,100]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hid, n_layers = 256, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb(wordvecs, itos, emb_sz):\n",
    "    emb= nn.Embedding(len(itos), emb_sz, padding_idx=1)\n",
    "    weights = emb.weight.data\n",
    "    not_found = []\n",
    "    \n",
    "    for idx, word in enumerate(itos):\n",
    "        try:\n",
    "            weights[idx] = torch.from_numpy(wordvecs[word] * 3)\n",
    "        except:\n",
    "            not_found.append(word)\n",
    "    print(len(not_found), not_found[5:10])\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class seq2seqAutoEncoder(nn.Module):\n",
    "    def __init__(self, device, wordvecs, itos, emb_sz, n_hid, out_seqlen, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.n_hid, self.n_layers, self.out_seqlen = n_hid, n_layers, out_seqlen\n",
    "        self.device = device\n",
    "        \n",
    "        # Encoder\n",
    "        self.emb = create_emb(wordvecs, itos, emb_sz)\n",
    "        self.emb = self.emb.to(self.device)\n",
    "        self.emb_drop = nn.Dropout(0.15)\n",
    "        self.rnn_enc = nn.GRU(emb_sz, n_hid, num_layers=n_layers, dropout=0.25)\n",
    "        self.output_enc = nn.Linear(n_hid, emb_sz, bias=False)\n",
    "        \n",
    "        # Decoder\n",
    "        self.rnn_dec = nn.GRU(emb_sz, emb_sz, num_layers=n_layers, dropout=0.1)  # square to enable weight tying\n",
    "        self.out_drop = nn.Dropout(0.35)\n",
    "        self.out = nn.Linear(emb_sz, len(itos))\n",
    "        self.out.weight.data = self.emb.weight.data\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        #pdb.set_trace()\n",
    "        seqlen, bs = inp.size()\n",
    "        \n",
    "        # Encode\n",
    "        h = self.initHidden(bs)\n",
    "        h = h.to(self.device)\n",
    "        emb = self.emb_drop(self.emb(inp))\n",
    "        enc_output, h = self.rnn_enc(emb, h) # h[1] is same as enc_outp[-1]!\n",
    "        \n",
    "        h = self.output_enc(h)\n",
    "        \n",
    "        # Decode\n",
    "        dec_inp = torch.zeros(bs, requires_grad=False).long()\n",
    "        dec_inp = dec_inp.to(self.device)\n",
    "        res = []\n",
    "        \n",
    "        for i in range(self.out_seqlen):\n",
    "            emb = self.emb(dec_inp).unsqueeze(0)  # adds unit axis at beginning so that rnn 'loops' once\n",
    "            output, h = self.rnn_dec(emb, h)\n",
    "            output = self.out(self.out_drop(output[0]))\n",
    "            res.append(output)\n",
    "            dec_inp = output.data.max(1)[1]  # [1] to get argmax\n",
    "            if (dec_inp == 1).all():\n",
    "                break\n",
    "        \n",
    "        return torch.stack(res)\n",
    "    \n",
    "    def initHidden(self, bs):\n",
    "        return torch.zeros(self.n_layers, bs, self.n_hid, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Seq2SeqLoss(pred, target):\n",
    "    seqlen, bs = target.size()\n",
    "    seqlen_pred, bs_pred, n_probs = pred.size()\n",
    "    \n",
    "    # we need to pad if target seqlen is larger than prediction seqlen\n",
    "    if seqlen > seqlen_pred:\n",
    "        pred = F.pad(pred, (0,0,0,0,0,seqlen-seqlen_pred))\n",
    "    \n",
    "    # but we only compare until the seqlen of the target\n",
    "    pred = pred[:seqlen]\n",
    "    \n",
    "    # cross_entropy can't handle rank 3 tensors currently, we need to flatten\n",
    "    return F.cross_entropy(pred.view(-1, n_probs), target.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_sz = vec_dict[\"cat\"].size\n",
    "emb_sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3689 ['xxup', 'xxrep', 'xxwrep', ':', \"'s\"]\n"
     ]
    }
   ],
   "source": [
    "rnn = seq2seqAutoEncoder(gpu, vec_dict, vocab.itos, emb_sz, n_hid, 30, n_layers)\n",
    "rnn = rnn.to(gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_fn = partial(optim.Adam, betas=(0.8, 0.99))  # like bind in c++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataBunch = DataBunch(train_dl=trn_dl, valid_dl=val_dl, device=gpu, path=seq_PATH, collate_fn=my_pad_collate_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(data=dataBunch, model=to_device(rnn, gpu), opt_func=opt_fn, loss_func=Seq2SeqLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    }
   ],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot(skip_end=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 3e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:imgcap]",
   "language": "python",
   "name": "conda-env-imgcap-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
