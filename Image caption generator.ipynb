{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image caption generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - seq2seq auto encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path('data/')\n",
    "PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_PATH = PATH/'seq2deq_model'\n",
    "seq_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_and_links_valid = pd.read_csv(\"Validation_GCC-1.1.0-Validation.tsv\", sep=\"\\t\",header=None)\n",
    "captions_and_links_train = pd.read_csv(\"Train_GCC-training.tsv\", sep=\"\\t\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15840, 3318333)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(captions_and_links_valid), len(captions_and_links_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>author : a life in photography -- in pictures</td>\n",
       "      <td>https://i.pinimg.com/736x/66/01/6c/66016c3ba27...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>an angler fishes river on a snowy day .</td>\n",
       "      <td>http://www.standard.net/image/2015/02/04/800x_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>photograph of the sign being repaired by brave...</td>\n",
       "      <td>http://indianapolis-photos.funcityfinder.com/f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the player staring intently at a computer scre...</td>\n",
       "      <td>http://www.abc.net.au/news/image/9066492-3x2-7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>globes : the green 3d person carrying in hands...</td>\n",
       "      <td>https://www.featurepics.com/StockImage/2009031...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0      author : a life in photography -- in pictures   \n",
       "1            an angler fishes river on a snowy day .   \n",
       "2  photograph of the sign being repaired by brave...   \n",
       "3  the player staring intently at a computer scre...   \n",
       "4  globes : the green 3d person carrying in hands...   \n",
       "\n",
       "                                                   1  \n",
       "0  https://i.pinimg.com/736x/66/01/6c/66016c3ba27...  \n",
       "1  http://www.standard.net/image/2015/02/04/800x_...  \n",
       "2  http://indianapolis-photos.funcityfinder.com/f...  \n",
       "3  http://www.abc.net.au/news/image/9066492-3x2-7...  \n",
       "4  https://www.featurepics.com/StockImage/2009031...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions_and_links_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_valid = captions_and_links_valid[0].values\n",
    "captions_train = captions_and_links_train[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3318333, 15840)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(captions_train), len(captions_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_captions_valid = tokenizer.process_all(captions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_captions_train = tokenizer.process_all(captions_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's make sure everything went right:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people click into their skis . people click into their skis .\n"
     ]
    }
   ],
   "source": [
    "print(captions_train[777], ' '.join(tokens_captions_train[777]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length of captions\n",
    "#### Average length of captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.415467171717172"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([len(c) for c in tokens_captions_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.322056888202601"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([len(c) for c in tokens_captions_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discard captions that are too long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_captions_valid = np.array(tokens_captions_valid)[np.array([len(c) < 30 for c in tokens_captions_valid])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max([len(c) for c in tokens_captions_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_captions_train = np.array(tokens_captions_train)[np.array([len(c) < 30 for c in tokens_captions_train])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max([len(c) for c in tokens_captions_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump((tokens_captions_valid), (seq_PATH/'tok_cap_v.pkl').open('wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump((tokens_captions_train), (seq_PATH/'tok_cap_t.pkl').open('wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_captions_valid = pickle.load((seq_PATH/'tok_cap_v.pkl').open('rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_captions_train = pickle.load((seq_PATH/'tok_cap_t.pkl').open('rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3302516 15764\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens_captions_train), len(tokens_captions_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numericalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab.create(tokens_captions_train, max_vocab=50000, min_freq=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's quickly test this:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 12575"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actor attending the world premiere of crime fiction film\n"
     ]
    }
   ],
   "source": [
    "test_caption = tokens_captions_valid[idx]\n",
    "print(' '.join(w for w in test_caption))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31, 1841, 8, 108, 43, 11, 2274, 1018, 76]\n"
     ]
    }
   ],
   "source": [
    "test_caption_num = vocab.numericalize(test_caption)\n",
    "print(test_caption_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actor attending the world premiere of crime fiction film\n"
     ]
    }
   ],
   "source": [
    "print(vocab.textify(test_caption_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['author', ':', 'a', 'life', 'in', 'photography', '--', 'in', 'pictures']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_captions_valid[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalize_tokens(tok):\n",
    "    return np.array([vocab.numericalize(q) + [1] for q in tok])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_numericalized_valid = numericalize_tokens(tokens_captions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_numericalized_train = numericalize_tokens(tokens_captions_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a vector illustration of a motorcycle contrasted against a bicycle . xxpad'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.textify(captions_numericalized_train[1256339])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install git+https://github.com/facebookresearch/fastText.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastText as ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vecs = ft.load_model(str(seq_PATH/'wiki.en.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_dict = {w : en_vecs.get_word_vector(w) for w in en_vecs.get_words()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(vec_dict, open(seq_PATH/'vec_dict.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_dict = pickle.load(open(seq_PATH/'vec_dict.pkl','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's take a look at the most frequent words from fastText:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_words = en_vecs.get_words(include_freq=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_word_dict = {k:v for k,v in zip(*ft_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_words = sorted(ft_word_dict.keys(), key=lambda x: ft_word_dict[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 most frequent words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[')', \"'\", 'and', 'in', '-', 'of', '</s>', 'the', '.', ',']"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_words[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mean and stdv of the word vectors:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs = np.stack(list(vec_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0075652334, 0.29283327)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs.mean(), vecs.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def A(*a):\n",
    "    \"\"\"convert iterable object into numpy array\"\"\"\n",
    "    return np.array(a[0]) if len(a)==1 else [np.array(o) for o in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Caption2CaptionDataset(Dataset):\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "    def __getitem__(self, idx):\n",
    "        return A(self.x[idx], self.x[idx])\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_ds = Caption2CaptionDataset(captions_numericalized_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = Caption2CaptionDataset(captions_numericalized_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3302516, 15764)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trn_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a very typical bus station xxpad'"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.textify(trn_ds[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_sampler = SortSampler(captions_numericalized_valid, key=lambda x:len(captions_numericalized_valid[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_sampler = SortishSampler(captions_numericalized_train, key=lambda x: len(captions_numericalized_train[x]), bs=bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to fully utilize the GPUs capabilities, we train in batches. The length of a minibatch tensor is set by the sequence length of the longest question in that batch. The other questions are padded. To save time and memory, we want to avoid very long and very short questions in one batch because that would mean lot's of padding. For the validation set we simply sort the questions. For training we use the `SortishSampler` which groups *longer* questions together and *shorter* questions together while preserving some randomness.\n",
    "\n",
    "For language models it's better to pad before the start of the sequence because we need the final hidden state to predict the next token or for classification...\n",
    "\n",
    "For sequence to sequence models it is better to pad after the end of the sequence.\n",
    "\n",
    "The samplers return an index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = next(iter(trn_sampler))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this tree , which fell only metres from a residence , was one of hundreds that suffered extensive damage at the hands of gusting winds on saturday afternoon . xxpad'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.textify(captions_numericalized_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_collate_seq2seq(samples:BatchSamples, pad_idx:int=1, pad_first:bool=True, backwards:bool=False, transpose:bool=False) -> Tuple[LongTensor, LongTensor]:\n",
    "    \"Function that collect samples and adds padding. Flips token order if needed\"\n",
    "    samples = to_data(samples)\n",
    "    max_len_inp = max([len(s[0]) for s in samples])\n",
    "    max_len_out = max([len(s[1]) for s in samples])\n",
    "    \n",
    "    res_inp = torch.zeros(len(samples), max_len_inp).long() + pad_idx\n",
    "    res_out = torch.zeros(len(samples), max_len_out).long() + pad_idx\n",
    "    \n",
    "    if backwards: pad_first = not pad_first\n",
    "    for i,s in enumerate(samples):\n",
    "        if pad_first: \n",
    "            res_inp[i,-len(s[0]):] = LongTensor(s[0])\n",
    "            res_out[i,-len(s[1]):] = LongTensor(s[1])\n",
    "        else:         \n",
    "            res_inp[i,:len(s[0]):] = LongTensor(s[0])\n",
    "            res_out[i,:len(s[1]):] = LongTensor(s[1])\n",
    "    if backwards:\n",
    "        res = res.flip(1)\n",
    "    if transpose:\n",
    "        res_inp.transpose_(0,1)\n",
    "        res_out.transpose_(0,1)\n",
    "    return res_inp, res_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jag_stack(self, b):\n",
    "    if len(b[0].shape) not in (1,2): return np.stack(b)\n",
    "    ml = max(len(o) for o in b)\n",
    "    if min(len(o) for o in b)==ml: return np.stack(b)\n",
    "    res = np.zeros((len(b), ml), dtype=b[0].dtype) + self.pad_idx\n",
    "    for i,o in enumerate(b):\n",
    "        if self.pre_pad: res[i, -len(o):] = o\n",
    "        else:            res[i,  :len(o)] = o\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pad_collate_func = partial(pad_collate_seq2seq, pad_first=False, transpose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dl = DataLoader(dataset=trn_ds, batch_size=bs, sampler=trn_sampler, collate_fn=my_pad_collate_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dl = DataLoader(dataset=val_ds, batch_size=bs, sampler=val_sampler, collate_fn=my_pad_collate_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the dataloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "its = [next(it) for _ in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(torch.Size([30, 125]), torch.Size([30, 125])),\n",
       " (torch.Size([27, 125]), torch.Size([27, 125])),\n",
       " (torch.Size([24, 125]), torch.Size([24, 125]))]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(x.shape, y.shape) for x, y in its]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is an old photo of my figure i had and i slightly edited it and now he looks like he 's from a dramatic anime xxpad xxpad xxpad xxpad\n",
      "this is an old photo of my figure i had and i slightly edited it and now he looks like he 's from a dramatic anime xxpad xxpad xxpad xxpad\n",
      "\n",
      "i have no idea how that american flag got in this picture of the construction , but i love that it is there ! xxpad xxpad xxpad\n",
      "i have no idea how that american flag got in this picture of the construction , but i love that it is there ! xxpad xxpad xxpad\n",
      "\n",
      "detail shot : the hands of a woman driving a car by the countryside , seen from the front passenger 's seat xxpad xxpad\n",
      "detail shot : the hands of a woman driving a car by the countryside , seen from the front passenger 's seat xxpad xxpad\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x, y in its:\n",
    "    print(vocab.textify(x[:,100]))\n",
    "    print(vocab.textify(y[:,100]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:imgcap]",
   "language": "python",
   "name": "conda-env-imgcap-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
